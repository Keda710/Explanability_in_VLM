{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11449863,"sourceType":"datasetVersion","datasetId":7173694},{"sourceId":11471743,"sourceType":"datasetVersion","datasetId":7189295},{"sourceId":11477063,"sourceType":"datasetVersion","datasetId":7193191}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade transformers==4.40.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T17:08:49.082254Z","iopub.execute_input":"2025-04-20T17:08:49.082936Z","iopub.status.idle":"2025-04-20T17:09:07.652809Z","shell.execute_reply.started":"2025-04-20T17:08:49.082909Z","shell.execute_reply":"2025-04-20T17:09:07.652112Z"}},"outputs":[{"name":"stdout","text":"Collecting transformers==4.40.0\n  Downloading transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (2.32.3)\nCollecting tokenizers<0.20,>=0.19 (from transformers==4.40.0)\n  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (2025.3.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.40.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.40.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.40.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.40.0) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.40.0) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.40.0) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.40.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.40.0) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.40.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.40.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.40.0) (2024.2.0)\nDownloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.0\n    Uninstalling tokenizers-0.21.0:\n      Successfully uninstalled tokenizers-0.21.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.51.1\n    Uninstalling transformers-4.51.1:\n      Successfully uninstalled transformers-4.51.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed tokenizers-0.19.1 transformers-4.40.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom PIL import Image\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport requests\nfrom io import BytesIO\n\n# Step 1: Setup constant\nmodel_name = \"StanfordAIMI/CheXagent-2-3b\"  # Pretrained model name\ndtype = torch.bfloat16  # Model's data type\ndevice = \"cuda\"  # Use GPU if available\n\n# Step 2: Load Processor and Model\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", trust_remote_code=True)\nmodel = model.to(dtype)\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T17:09:07.654500Z","iopub.execute_input":"2025-04-20T17:09:07.655142Z","iopub.status.idle":"2025-04-20T17:11:32.950570Z","shell.execute_reply.started":"2025-04-20T17:09:07.655121Z","shell.execute_reply":"2025-04-20T17:11:32.949893Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/21.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8da648b0dab4a7f8cf28d3592644b64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenization_chexagent.py:   0%|          | 0.00/26.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e53f191d86a84b07b9983f9638f2270b"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/StanfordAIMI/CheXagent-2-3b:\n- tokenization_chexagent.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/999k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76fce5134ab6455bb9b99c33d3f9aa86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb032eaf6db34a61b3cdaf8097897630"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/2.85k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f31a0d349fd549efad5b76be9194e749"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/769 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed68b4e5cb6141b29c52b075e1c7d276"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"031659e9135046a8bb5be87e788951e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_chexagent.py:   0%|          | 0.00/9.35k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a7590487abd4767999499abd131e29a"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/StanfordAIMI/CheXagent-2-3b:\n- configuration_chexagent.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_chexagent.py:   0%|          | 0.00/53.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d01d10d98c344c01a2e7e171ed909cb4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modeling_visual.py:   0%|          | 0.00/8.24k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9a001d0efbc47f5ab734add9c23cfc9"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.5' (you have '2.0.4'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\nA new version of the following files was downloaded from https://huggingface.co/StanfordAIMI/CheXagent-2-3b:\n- modeling_visual.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/StanfordAIMI/CheXagent-2-3b:\n- modeling_chexagent.py\n- modeling_visual.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n2025-04-20 17:09:32.622989: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745168973.090144      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745168973.231694      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/75.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a668acd689244eeab3f1a21d55c7b309"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee3ddfbe569e478fa12bd95fc5d56ac1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3913b50a54714ee0a14ea1c4310bc285"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff1b6a26b7e6448b9e0e6f5cec864e2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/2.60G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c85705e254f4aa28206cc1365c4926d"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5024629de54f490c91cadd9fa8e6d133"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.61G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"594b96548d7542839616c658b2fcbf37"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for logit_scale: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for logit_bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.embeddings.token_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.embeddings.position_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.0.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.0.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.0.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.0.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.0.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.0.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.0.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.0.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.0.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.0.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.0.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.0.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.0.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.1.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.1.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.1.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.1.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.1.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.1.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.1.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.1.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.1.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.1.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.1.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.1.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.1.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.2.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.2.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.2.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.2.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.2.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.2.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.2.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.2.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.2.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.2.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.2.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.2.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.2.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.3.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.3.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.3.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.3.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.3.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.3.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.3.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.3.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.3.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.3.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.3.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.3.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.3.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.4.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.4.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.4.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.4.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.4.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.4.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.4.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.4.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.4.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.4.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.4.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.4.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.4.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.5.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.5.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.5.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.5.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.5.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.5.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.5.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.5.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.5.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.5.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.5.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.5.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.5.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.6.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.6.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.6.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.6.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.6.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.6.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.6.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.6.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.6.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.6.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.6.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.6.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.6.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.7.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.7.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.7.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.7.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.7.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.7.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.7.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.7.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.7.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.7.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.7.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.7.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.7.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.8.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.8.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.8.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.8.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.8.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.8.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.8.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.8.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.8.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.8.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.8.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.8.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.8.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.9.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.9.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.9.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.9.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.9.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.9.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.9.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.9.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.9.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.9.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.9.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.9.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.9.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.10.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.10.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.10.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.10.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.10.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.10.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.10.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.10.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.10.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.10.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.10.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.10.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.10.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.11.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.11.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.11.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.11.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.11.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.11.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.11.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.11.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.11.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.11.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.11.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.11.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.11.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.12.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.12.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.12.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.12.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.12.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.12.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.12.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.12.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.12.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.12.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.12.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.12.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.12.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.13.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.13.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.13.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.13.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.13.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.13.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.13.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.13.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.13.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.13.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.13.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.13.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.13.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.14.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.14.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.14.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.14.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.14.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.14.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.14.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.14.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.14.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.14.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.14.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.14.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.14.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.15.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.15.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.15.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.15.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.15.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.15.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.15.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.15.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.15.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.15.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.15.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.15.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.15.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.16.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.16.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.16.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.16.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.16.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.16.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.16.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.16.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.16.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.16.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.16.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.16.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.16.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.17.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.17.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.17.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.17.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.17.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.17.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.17.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.17.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.17.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.17.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.17.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.17.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.17.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.18.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.18.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.18.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.18.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.18.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.18.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.18.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.18.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.18.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.18.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.18.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.18.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.18.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.19.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.19.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.19.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.19.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.19.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.19.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.19.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.19.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.19.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.19.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.19.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.19.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.19.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.20.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.20.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.20.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.20.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.20.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.20.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.20.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.20.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.20.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.20.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.20.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.20.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.20.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.21.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.21.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.21.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.21.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.21.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.21.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.21.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.21.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.21.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.21.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.21.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.21.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.21.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.22.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.22.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.22.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.22.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.22.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.22.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.22.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.22.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.22.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.22.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.22.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.22.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.22.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.23.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.23.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.23.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.23.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.23.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.23.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.23.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.23.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.23.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.23.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.23.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.23.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.23.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.final_layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.final_layer_norm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.head.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.head.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.embeddings.patch_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.embeddings.patch_embedding.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.embeddings.position_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.0.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.0.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.0.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.0.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.1.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.1.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.1.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.1.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.2.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.2.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.2.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.2.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.3.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.3.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.3.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.3.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.4.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.4.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.4.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.4.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.5.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.5.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.5.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.5.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.6.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.6.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.6.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.6.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.7.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.7.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.7.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.7.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.8.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.8.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.8.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.8.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.9.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.9.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.9.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.9.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.10.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.10.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.10.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.10.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.11.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.11.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.11.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.11.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.12.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.12.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.12.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.12.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.13.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.13.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.13.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.13.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.14.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.14.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.14.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.14.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.15.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.15.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.15.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.15.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.16.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.16.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.16.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.16.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.17.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.17.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.17.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.17.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.18.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.18.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.18.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.18.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.19.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.19.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.19.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.19.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.20.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.20.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.20.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.20.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.21.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.21.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.21.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.21.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.22.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.22.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.22.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.22.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.23.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.23.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.23.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.encoder.layers.23.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.post_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.post_layernorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.head.probe: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.head.attention.in_proj_weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.head.attention.in_proj_bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.head.attention.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.head.attention.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.head.layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.head.layernorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.head.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.head.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.head.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for vision_model.head.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/368 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee581a1c85d04f8b91cb78db3191a6e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/711 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6257b7bf261440e3acca0b47df560d7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79aa152ed4a34e069f247c28dcfe6247"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/409 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"268af934537241c69eabd06bce0ac06b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3555430dea034b318fd8b762b8dad949"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c56b4cd53664b0b8aa167c004df49db"}},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"CheXagentForCausalLM(\n  (model): CheXagentModel(\n    (embed_tokens): Embedding(51200, 2560)\n    (embed_dropout): Dropout(p=0.0, inplace=False)\n    (layers): ModuleList(\n      (0-31): 32 x PhiDecoderLayer(\n        (self_attn): PhiAttention(\n          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n          (dense): Linear(in_features=2560, out_features=2560, bias=True)\n          (rotary_emb): PhiRotaryEmbedding()\n        )\n        (mlp): PhiMLP(\n          (activation_fn): NewGELUActivation()\n          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n        )\n        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n    (visual): CLIPModel(\n      (model): SiglipVisionTransformer(\n        (embeddings): SiglipVisionEmbeddings(\n          (patch_embedding): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16), padding=valid)\n          (position_embedding): Embedding(1024, 1024)\n        )\n        (encoder): SiglipEncoder(\n          (layers): ModuleList(\n            (0-23): 24 x SiglipEncoderLayer(\n              (self_attn): SiglipAttention(\n                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              )\n              (layer_norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n              (mlp): SiglipMLP(\n                (activation_fn): GELUActivation()\n                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n              )\n              (layer_norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n            )\n          )\n        )\n        (post_layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (head): SiglipMultiheadAttentionPoolingHead(\n          (attention): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): SiglipMLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          )\n        )\n      )\n      (attn_pool): Sequential(\n        (0): Linear(in_features=1024, out_features=10240, bias=True)\n        (1): GELUActivation()\n        (2): Linear(in_features=10240, out_features=2560, bias=True)\n      )\n      (ln_post): LayerNorm((2560,), eps=1e-06, elementwise_affine=True)\n    )\n  )\n  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n)"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# Step 3: Load Image\nimage_path = \"/kaggle/input/padchest/173480850478104643017119358503404014196_qey2vk.png\"  # Replace with the path to your image\nimage = Image.open(image_path)\n\n# Replace with the text prompt you'd like to send\ndef output(image_path,image) :\n    # Step 4: Preprocess Image and Text Prompt\n    text_prompt = \"Describe the chest X-ray image with respect to: Airway and mediastinum ,Lung fields (including infiltrates, nodules, effusion),Cardiac silhouette ,Bones and soft tissues,Devices or foreign objects,Provide a radiologist-style report.Alos tell what unsual things you see there.\"\n    \n    # Assuming the tokenizer's from_list_format expects a list of images and text, format them correctly:\n    # Convert the image to a list-like structure\n    image_data = {'image': image_path}  # The image path or the image object, as expected by the model\n    \n    # Prepare the query with the image and text pair\n    query = tokenizer.from_list_format([image_data, {'text': text_prompt}])\n    \n    # Step 5: Prepare the conversation for the model (system and human format)\n    conv = [\n        {\"from\": \"system\", \"value\": \"You are a helpful assistant.\"},\n        {\"from\": \"human\", \"value\": query}\n    ]\n    \n    # Step 6: Convert the conversation to input_ids\n    input_ids = tokenizer.apply_chat_template(conv, add_generation_prompt=True, return_tensors=\"pt\")\n    \n    # Step 7: Perform Inference\n    # Generate output from the model\n    output = model.generate(\n        input_ids.to(device), \n        do_sample=False, \n        num_beams=1, \n        temperature=1., \n        top_p=1., \n        use_cache=True,\n        max_new_tokens=512\n    )[0]\n    \n    # Step 8: Decode the model's output to get the response\n    response = tokenizer.decode(output[input_ids.size(1):-1])\n    # Step 9: Print the response\n    print(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T17:11:32.951358Z","iopub.execute_input":"2025-04-20T17:11:32.951901Z","iopub.status.idle":"2025-04-20T17:11:33.055694Z","shell.execute_reply.started":"2025-04-20T17:11:32.951879Z","shell.execute_reply":"2025-04-20T17:11:33.055208Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def output(image_path, image):\n    import torch\n\n    # Descriptive radiology-style prompt\n    text_prompt = (\n        \"Describe the chest X-ray image with respect to: \"\n        \"Airway and mediastinum, lung fields (including infiltrates, nodules, effusion), \"\n        \"cardiac silhouette, bones and soft tissues, and any visible devices or foreign objects. \"\n        \"Provide a radiologist-style report. Also mention any unusual findings.\"\n    )\n\n    # Pass the image path string, not the image object\n    query = tokenizer.from_list_format([\n        {'image': image_path},\n        {'text': text_prompt}\n    ])\n\n    # Prepare the chat format\n    conv = [\n        {\"from\": \"system\", \"value\": \"You are a helpful radiology assistant.\"},\n        {\"from\": \"human\", \"value\": query}\n    ]\n\n    # Encode conversation\n    input_ids = tokenizer.apply_chat_template(conv, add_generation_prompt=True, return_tensors=\"pt\").to(device)\n\n    # Generate response\n    with torch.no_grad():\n        output = model.generate(\n            input_ids=input_ids,\n            do_sample=False,\n            num_beams=1,\n            temperature=1.0,\n            top_p=1.0,\n            use_cache=True,\n            max_new_tokens=512\n        )[0]\n\n    # Decode generated part only\n    generated_text = tokenizer.decode(output[input_ids.size(1):], skip_special_tokens=True)\n\n    # Print the output\n    print(f\"Image: {image_path}\")\n    print(\"Generated Report:\\n\")\n    print(generated_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T17:11:33.057352Z","iopub.execute_input":"2025-04-20T17:11:33.057556Z","iopub.status.idle":"2025-04-20T17:11:33.062831Z","shell.execute_reply.started":"2025-04-20T17:11:33.057540Z","shell.execute_reply":"2025-04-20T17:11:33.062311Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"output(image_path,image)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T17:11:33.063627Z","iopub.execute_input":"2025-04-20T17:11:33.064215Z","iopub.status.idle":"2025-04-20T17:11:38.366599Z","shell.execute_reply.started":"2025-04-20T17:11:33.064190Z","shell.execute_reply":"2025-04-20T17:11:38.365724Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Image: /kaggle/input/padchest/173480850478104643017119358503404014196_qey2vk.png\nGenerated Report:\n\nNo abnormalities detected.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"image_p1 = '/kaggle/input/padchest/47513502650936350354964244779459913004-3_th1gik.png'\nimage1 = Image.open(image_path)\noutput(image_p1,image1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T17:11:38.367352Z","iopub.execute_input":"2025-04-20T17:11:38.367820Z","iopub.status.idle":"2025-04-20T17:11:50.410804Z","shell.execute_reply.started":"2025-04-20T17:11:38.367800Z","shell.execute_reply":"2025-04-20T17:11:50.410129Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Image: /kaggle/input/padchest/47513502650936350354964244779459913004-3_th1gik.png\nGenerated Report:\n\n<|ref|> foreign object <|/ref|> <|box|> (1,1),(24,12) <|/box|> <|ref|> foreign object <|/ref|> <|box|> (33,1),(70,12) <|/box|> <|ref|> foreign object <|/ref|> <|box|> (33,33),(70,48) <|/box|> <|ref|> foreign object <|/ref|> <|box|> (33,50),(70,68) <|/box|> <|ref|> foreign object <|/ref|> <|box|> (33,80),(70,99) <|/box|> <|ref|> foreign object <|/ref|> <|box|> (33,85),(70,99) <|/box|> <|ref|> foreign object <|/ref|> <|box|> (33,70),(70,99) <|/box|> <|ref|> foreign object <|/ref|> <|box|> (33,50),(70,68) <|/box|> <|ref|> foreign object <|/ref|> <|box|> (33,33),(70,48) <|/box|> <|ref|> foreign object <|/ref|> <|box|> (33,50),(70,68) <|/box|> <|ref|> foreign object <|/ref|> <|box|> (33,80),(70,99) <|/box|> <|ref|> foreign object <|/ref|> <|box|> (33,85),(70,99) <|/box|>\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import os\n# Path to the directory containing the images\nimage_dir = \"/kaggle/input/padchest2/\"  # example path\nimage_extensions = ['.png', '.jpg', '.jpeg']  # valid image formats\n\n# Load all image paths\nimage_paths = [os.path.join(image_dir, file)\n               for file in os.listdir(image_dir)\n               if os.path.splitext(file)[1].lower() in image_extensions]\n\n# Load images using PIL and convert to RGB\nimages = [Image.open(path).convert(\"RGB\") for path in image_paths]\nn = len(image_paths)\nfor i in range(n) :\n    output(image_paths[i],images[i])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T17:11:50.411534Z","iopub.execute_input":"2025-04-20T17:11:50.411768Z","iopub.status.idle":"2025-04-20T17:12:27.695550Z","shell.execute_reply.started":"2025-04-20T17:11:50.411741Z","shell.execute_reply":"2025-04-20T17:12:27.694877Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Image: /kaggle/input/padchest2/287674558154154196238750035876376456720_de0kdu.png\nGenerated Report:\n\n[Breathing: Lungs] The lung fields are clear. [Breathing: Pleura] There is no pneumothorax. [Cardiac: Heart Size and Borders] The cardiac silhouette is within normal limits. [Everything else: Bones and Soft Tissues] There are no osseous or soft tissue abnormalities.\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Image: /kaggle/input/padchest2/120641250378848817226599591125249193787_1vwu72.png\nGenerated Report:\n\n[Breathing: Lungs] The lung fields are clear. [Breathing: Pleura] There is no pneumothorax. [Cardiac: Heart Size and Borders] The heart and mediastinum are within normal limits. [Everything else: Bones and Soft Tissues] The bones and soft tissues are unremarkable.\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Image: /kaggle/input/padchest2/117503487105610561494614672580071778723_egzn5w.png\nGenerated Report:\n\n[Breathing: Lungs] The lung fields are clear. [Breathing: Pleura] There is no pneumothorax. [Cardiac: Heart Size and Borders] The cardiomediastinal silhouette is within normal limits. [Everything else: Bones] There are no acute osseous abnormalities.\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Image: /kaggle/input/padchest2/104055072637907464008767269203861924017_huh41k.png\nGenerated Report:\n\n[Everything else: Foreign Body] There is no radiopaque foreign body. [Breathing: Lungs] The lungs are clear. [Cardiac: Heart Size and Borders] The cardiomediastinal silhouette is normal. [Breathing: Pleura] There is no pleural effusion or pneumothorax.\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Image: /kaggle/input/padchest2/235853514537661378209836459660303802572_-bl64a.png\nGenerated Report:\n\n[Breathing: Lungs] The lung fields are clear. [Breathing: Pleura] There is no pneumothorax. [Cardiac: Heart Size and Borders] The heart and mediastinum are within normal limits. [Everything else: Bones and Soft Tissues] The bones and soft tissues are unremarkable.\nImage: /kaggle/input/padchest2/157943178667978315593423977854134065828_w1j177.png\nGenerated Report:\n\n<|ref|> Airway and mediastinal contours are within normal limits.**\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# import os\n# # Path to the directory containing the images\n# image_dir = \"/kaggle/input/padchest/\"  # example path\n# image_extensions = ['.png', '.jpg', '.jpeg']  # valid image formats\n\n# # Load all image paths\n# image_paths = [os.path.join(image_dir, file)\n#                for file in os.listdir(image_dir)\n#                if os.path.splitext(file)[1].lower() in image_extensions]\n\n# # Load images using PIL and convert to RGB\n# images = [Image.open(path).convert(\"RGB\") for path in image_paths]\n# n = len(image_paths)\n# for i in range(n) :\n#     output(image_paths[i],images[i])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T17:12:27.696472Z","iopub.execute_input":"2025-04-20T17:12:27.696731Z","iopub.status.idle":"2025-04-20T17:12:27.700813Z","shell.execute_reply.started":"2025-04-20T17:12:27.696700Z","shell.execute_reply":"2025-04-20T17:12:27.699771Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# def output(image_path, image):\n#     import torch\n\n#     # Descriptive radiology-style prompt\n#     text_prompt = (\n#         \"Describe the chest X-ray image with respect to: \"\n#         \"Airway and mediastinum, lung fields (including infiltrates, nodules, effusion), \"\n#         \"cardiac silhouette, bones and soft tissues, and any visible devices or foreign objects. \"\n#         \"Provide a radiologist-style report. Also mention any unusual findings.\"\n#     )\n\n#     # Pass the image path string, not the image object\n#     query = tokenizer.from_list_format([\n#         {'image': image_path},\n#         {'text': text_prompt}\n#     ])\n\n#     # Prepare the chat format\n#     conv = [\n#         {\"from\": \"system\", \"value\": \"You are a helpful radiology assistant.\"},\n#         {\"from\": \"human\", \"value\": query}\n#     ]\n\n#     # Encode conversation\n#     input_ids = tokenizer.apply_chat_template(conv, add_generation_prompt=True, return_tensors=\"pt\").to(device)\n\n#     # Generate response\n#     with torch.no_grad():\n#         output = model.generate(\n#             input_ids=input_ids,\n#             do_sample=False,\n#             num_beams=1,\n#             temperature=1.0,\n#             top_p=1.0,\n#             use_cache=True,\n#             max_new_tokens=512\n#         )[0]\n\n#     # Decode generated part only\n#     generated_text = tokenizer.decode(output[input_ids.size(1):], skip_special_tokens=True)\n\n#     # Print the output\n#     print(f\"Image: {image_path}\")\n#     print(\"Generated Report:\\n\")\n#     print(generated_text)\n#     output_path = \"/kaggle/input/out_chestx-det10/\" \n#     draw_ans_save_boxes(image_path,generated_text,output_path)\n\n\n\n# import os\n# # Path to the directory containing the images\n# image_dir = \"/kaggle/input/chestx-det10/\"  # example path\n# image_extensions = ['.png', '.jpg', '.jpeg']  # valid image formats\n\n# # Load all image paths\n# image_paths = [os.path.join(image_dir, file)\n#                for file in os.listdir(image_dir)\n#                if os.path.splitext(file)[1].lower() in image_extensions]\n\n# # Load images using PIL and convert to RGB\n# images = [Image.open(path).convert(\"RGB\") for path in image_paths]\n# n = len(image_paths)\n# for i in range(n) :\n#     output(image_paths[i],images[i])\n# import cv2\n# import re\n\n# def draw_and_save_boxes(image_path, report, output_path):\n#     # Load the image\n#     image = cv2.imread(image_path)\n#     if image is None:\n#         print(f\"Could not load image: {image_path}\")\n#         return\n\n#     # Find all box coordinates\n#     boxes = re.findall(r\"<\\|box\\|>\\s*\\((\\d+),(\\d+)\\),\\((\\d+),(\\d+)\\)\\s*<\\|/box\\|>\", report)\n    \n#     for box in boxes:\n#         x1, y1, x2, y2 = map(int, box)\n#         cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Green box with thickness 2\n\n#     # Save the image with boxes\n#     cv2.imwrite(output_path, image)\n#     print(f\"Saved boxed image to {output_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T17:12:27.701645Z","iopub.execute_input":"2025-04-20T17:12:27.701922Z","iopub.status.idle":"2025-04-20T17:12:27.720985Z","shell.execute_reply.started":"2025-04-20T17:12:27.701898Z","shell.execute_reply":"2025-04-20T17:12:27.720333Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import os\nimport re\nimport cv2\nimport torch\nfrom PIL import Image\n\n# Make sure this directory exists; if not, create it\noutput_dir = \"/kaggle/working/out_chestx-det10\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Function to draw boxes and save the image\ndef draw_and_save_boxes(image_path, report, output_dir):\n    # Load the image using OpenCV\n    image = cv2.imread(image_path)\n    if image is None:\n        print(f\"Could not load image: {image_path}\")\n        return\n\n    height, width = image.shape[:2]  # Actual image size\n\n    # Find all normalized box coordinates (in 0–100 range)\n    boxes = re.findall(r\"<\\|box\\|>\\s*\\((\\d+),(\\d+)\\),\\((\\d+),(\\d+)\\)\\s*<\\|/box\\|>\", report)\n\n    for box in boxes:\n        # Normalize coordinates back to pixel values\n        x1, y1, x2, y2 = map(int, box)\n        x1 = int((x1 / 100) * width)\n        y1 = int((y1 / 100) * height)\n        x2 = int((x2 / 100) * width)\n        y2 = int((y2 / 100) * height)\n\n        # Draw rectangle\n        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Green box\n\n    # Save output image\n    filename = os.path.basename(image_path)\n    output_path = os.path.join(output_dir, filename)\n    cv2.imwrite(output_path, image)\n    print(f\"Saved boxed image to {output_path}\")\n\n\n# Main function to process an image\ndef output(image_path, image):\n    # Descriptive radiology-style prompt\n    text_prompt = (\n        \"Describe the chest X-ray image with respect to: \"\n        \"Airway and mediastinum, lung fields (including infiltrates, nodules, effusion), \"\n        \"cardiac silhouette, bones and soft tissues, and any visible devices or foreign objects. \"\n        \"Provide a radiologist-style report. Also mention any unusual findings.\"\n    )\n\n    # Format input for model\n    query = tokenizer.from_list_format([\n        {'image': image_path},  # pass image path as string\n        {'text': text_prompt}\n    ])\n\n    # Chat-style conversation\n    conv = [\n        {\"from\": \"system\", \"value\": \"You are a helpful radiology assistant.\"},\n        {\"from\": \"human\", \"value\": query}\n    ]\n\n    # Tokenize and move to device\n    input_ids = tokenizer.apply_chat_template(conv, add_generation_prompt=True, return_tensors=\"pt\").to(device)\n\n    # Generate response\n    with torch.no_grad():\n        output_tokens = model.generate(\n            input_ids=input_ids,\n            do_sample=False,\n            num_beams=1,\n            temperature=1.0,\n            top_p=1.0,\n            use_cache=True,\n            max_new_tokens=512\n        )[0]\n\n    # Decode generated output\n    generated_text = tokenizer.decode(output_tokens[input_ids.size(1):], skip_special_tokens=True)\n\n    # Print report\n    print(f\"\\nImage: {image_path}\")\n    print(\"Generated Report:\\n\")\n    print(generated_text)\n\n    # Draw and save box image if boxes are present\n    draw_and_save_boxes(image_path, generated_text, output_dir)\n\n# Set image directory path\nimage_dir = \"/kaggle/input/chestx-det10\"\nimage_extensions = ['.png', '.jpg', '.jpeg']\n\n# Get list of image paths\nimage_paths = [os.path.join(image_dir, file)\n               for file in os.listdir(image_dir)\n               if os.path.splitext(file)[1].lower() in image_extensions]\n\n# Convert images to PIL RGB format (optional, based on your model needs)\nimages = [Image.open(path).convert(\"RGB\") for path in image_paths]\n\n# Run on all images\nfor i in range(len(image_paths)):\n    output(image_paths[i], images[i])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T17:21:02.649557Z","iopub.execute_input":"2025-04-20T17:21:02.649844Z","iopub.status.idle":"2025-04-20T17:34:11.100701Z","shell.execute_reply.started":"2025-04-20T17:21:02.649822Z","shell.execute_reply":"2025-04-20T17:34:11.099961Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41463.png\nGenerated Report:\n\n<|ref|> Infiltration <|/ref|> <|box|> (14,50),(30,72) <|/box|> <|ref|> Infiltration <|/ref|> <|box|> (76,61),(90,73) <|/box|> <|ref|> Lung Opacity <|/ref|> <|box|> (76,61),(90,73) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41463.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41141.png\nGenerated Report:\n\n<|ref|> Lung Opacity <|/ref|> <|box|> (63,32),(73,40) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (64,32),(73,40) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41141.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41504.png\nGenerated Report:\n\n<|ref|> Infiltration <|/ref|> <|box|> (14,28),(43,77) <|/box|> <|ref|> Pulmonary fibrosis <|/ref|> <|box|> (64,16),(86,47) <|/box|> <|ref|> Pulmonary fibrosis <|/ref|> <|box|> (70,50),(80,64) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41504.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40050.png\nGenerated Report:\n\n<|ref|> Airway and mediastinum <|/ref|> <|box|> (33,30),(46,45) <|/box|> <|ref|> Lungs <|/ref|> <|box|> (33,30),(46,45) <|/box|> <|ref|> Cardiac silhouette <|/ref|> <|box|> (35,54),(75,68) <|/box|> <|ref|> Bones and soft tissues <|/ref|> <|box|> (35,54),(75,68) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40050.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41225.png\nGenerated Report:\n\n<|ref|> Airway and mediastinum <|/ref|> <|box|> (38,20),(48,31) <|/box|> <|ref|> Lung fields <|/ref|> <|box|> (38,20),(48,31) <|/box|> <|ref|> Cardiac silhouette <|/ref|> <|box|> (39,47),(76,63) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41225.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41757.png\nGenerated Report:\n\n<|ref|> Lung Opacity <|/ref|> <|box|> (60,33),(85,52) <|/box|> <|ref|> Pleural Effusion <|/ref|> <|box|> (63,38),(85,52) <|/box|> <|ref|> Cardiomegaly <|/ref|> <|box|> (64,33),(85,52) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41757.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/36212.png\nGenerated Report:\n\n<|ref|> Cardiomegaly <|/ref|> <|box|> (32,54),(84,82) <|/box|> <|ref|> Aortic enlargement <|/ref|> <|box|> (50,29),(63,42) <|/box|> <|ref|> Lung Opacity <|/ref|> <|box|> (56,20),(68,34) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (56,20),(68,34) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/36212.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40058.png\nGenerated Report:\n\n<|ref|> Lung Opacity <|/ref|> <|box|> (8,48),(40,66) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (8,48),(40,66) <|/box|> <|ref|> Cardiomegaly <|/ref|> <|box|> (30,50),(76,66) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (30,50),(76,66) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40058.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41130.png\nGenerated Report:\n\nNo abnormalities detected.\nSaved boxed image to /kaggle/working/out_chestx-det10/41130.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41479.png\nGenerated Report:\n\n<|ref|> Airway and mediastinal silhouette, lung fields <|/ref|> <|box|> (47,28),(64,47) <|/box|> <|ref|> Cardiac silhouette <|/ref|> <|box|> (47,28),(64,47) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41479.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41136.png\nGenerated Report:\n\n<|ref|> Infiltration <|/ref|> <|box|> (14,30),(32,44) <|/box|> <|ref|> Lung Opacity <|/ref|> <|box|> (14,30),(32,44) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (14,30),(32,44) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (64,16),(86,33) <|/box|> <|ref|> Lung Opacity <|/ref|> <|box|> (64,16),(86,33) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41136.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/39504.png\nGenerated Report:\n\n<|ref|> Pleural thickening <|/ref|> <|box|> (6,54),(38,68) <|/box|> <|ref|> Pleural effusion <|/ref|> <|box|> (6,54),(38,68) <|/box|> <|ref|> Pulmonary fibrosis <|/ref|> <|box|> (14,34),(38,52) <|/box|> <|ref|> Pulmonary fibrosis <|/ref|> <|box|> (20,28),(30,36) <|/box|> <|ref|> Cardiomegaly <|/ref|> <|box|> (38,54),(78,66) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (38,28),(43,35) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (38,28),(43,35) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/39504.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/36302.png\nGenerated Report:\n\n[Everything else: Tubes] The right PICC line is in place. [Breathing: Pleura] **There is a small left pleural effusion.** [Breathing: Lungs] There is no focal consolidation. [Breathing: Pleura] There is no pneumothorax. [Cardiac: Heart Size] The heart size is normal. [Everything else: Mediastinal] The mediastinal contours are normal. [Airway: Hilar Structures] The hilar contours are normal.\nSaved boxed image to /kaggle/working/out_chestx-det10/36302.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40476.png\nGenerated Report:\n\n[Everything else: Soft Tissues] **There are extensive surgical clips in the right axilla.** [Breathing: Lungs] There is no evidence of acute pneumonia, vascular congestion, or pleural effusion. [Breathing: Lungs] **A granuloma is seen at the left base.**\nSaved boxed image to /kaggle/working/out_chestx-det10/40476.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40367.png\nGenerated Report:\n\n<|ref|> Airway and mediastinum <|/ref|> <|box|> (44,20),(56,34) <|/box|> <|ref|> Lung fields <|/ref|> <|box|> (68,42),(83,64) <|/box|> <|ref|> Cardiac silhouette <|/ref|> <|box|> (47,50),(78,64) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40367.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41767.png\nGenerated Report:\n\n<|ref|> Infiltration <|/ref|> <|box|> (60,44),(80,60) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41767.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41220.png\nGenerated Report:\n\n<|ref|> Lung Opacity <|/ref|> <|box|> (14,42),(40,56) <|/box|> <|ref|> Pleural Effusion <|/ref|> <|box|> (14,42),(40,56) <|/box|> <|ref|> Cardiomegaly <|/ref|> <|box|> (24,41),(72,59) <|/box|> <|ref|> Aortic enlargement <|/ref|> <|box|> (34,26),(60,46) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41220.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/39956.png\nGenerated Report:\n\n<|ref|> Cardiomegaly <|/ref|> <|box|> (33,50),(85,68) <|/box|> <|ref|> Aortic enlargement <|/ref|> <|box|> (48,27),(66,44) <|/box|> <|ref|> Pleural thickening <|/ref|> <|box|> (55,12),(69,18) <|/box|> <|ref|> Pulmonary fibrosis <|/ref|> <|box|> (77,38),(84,45) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/39956.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41240.png\nGenerated Report:\n\n<|ref|> Infiltration <|/ref|> <|box|> (14,38),(40,73) <|/box|> <|ref|> Lung Opacity <|/ref|> <|box|> (14,38),(40,73) <|/box|> <|ref|> Cardiomegaly <|/ref|> <|box|> (34,54),(78,70) <|/box|> <|ref|> Aortic enlargement <|/ref|> <|box|> (48,32),(62,46) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41240.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/39502.png\nGenerated Report:\n\n<|ref|> Lung Opacity <|/ref|> <|box|> (14,52),(38,63) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (14,52),(38,63) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (69,36),(90,56) <|/box|> <|ref|> Lung Opacity <|/ref|> <|box|> (69,36),(90,56) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/39502.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41096.png\nGenerated Report:\n\nNo abnormalities detected.\nSaved boxed image to /kaggle/working/out_chestx-det10/41096.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/39796.png\nGenerated Report:\n\n<|ref|> Infiltration <|/ref|> <|box|> (14,26),(40,56) <|/box|> <|ref|> Mediastinal shift <|/ref|> <|box|> (40,14),(54,36) <|/box|> <|ref|> Infiltration <|/ref|> <|box|> (60,24),(85,56) <|/box|> <|ref|> Mediastinal shift <|/ref|> <|box|> (60,14),(73,36) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/39796.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40851.png\nGenerated Report:\n\n<|ref|> Lung Opacity <|/ref|> <|box|> (14,34),(40,48) <|/box|> <|ref|> Infiltration <|/ref|> <|box|> (14,34),(40,48) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (14,34),(40,48) <|/box|> <|ref|> Pleural thickening <|/ref|> <|box|> (14,34),(40,48) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40851.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/39431.png\nGenerated Report:\n\n<|ref|> Infiltration <|/ref|> <|box|> (14,24),(40,60) <|/box|> <|ref|> Consolidation <|/ref|> <|box|> (14,24),(40,60) <|/box|> <|ref|> Lung Opacity <|/ref|> <|box|> (14,24),(40,60) <|/box|> <|ref|> Consolidation <|/ref|> <|box|> (14,24),(40,60) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/39431.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40898.png\nGenerated Report:\n\nNo abnormalities detected.\nSaved boxed image to /kaggle/working/out_chestx-det10/40898.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/39412.png\nGenerated Report:\n\n<|ref|> Lung Opacity <|/ref|> <|box|> (14,48),(34,60) <|/box|> <|ref|> Lung Opacity <|/ref|> <|box|> (69,47),(86,60) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/39412.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40759.png\nGenerated Report:\n\n<|ref|> Lung Opacity <|/ref|> <|box|> (14,48),(40,60) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (14,48),(40,60) <|/box|> <|ref|> Cardiomegaly <|/ref|> <|box|> (39,48),(81,63) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40759.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41752.png\nGenerated Report:\n\n<|ref|> Airway and mediastinal shift <|/ref|> <|box|> (34,1),(52,33) <|/box|> <|ref|> Lung Opacity <|/ref|> <|box|> (34,1),(52,33) <|/box|> <|ref|> Pulmonary fibrosis <|/ref|> <|box|> (34,1),(52,33) <|/box|> <|ref|> Pulmonary fibrosis <|/ref|> <|box|> (70,20),(90,47) <|/box|> <|ref|> Pleural thickening <|/ref|> <|box|> (80,12),(90,20) <|/box|> <|ref|> Pleural thickening <|/ref|> <|box|> (89,80),(94,87) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41752.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40731.png\nGenerated Report:\n\n<|ref|> Infiltration <|/ref|> <|box|> (76,61),(89,71) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40731.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40078.png\nGenerated Report:\n\n<|ref|> Cardiomegaly <|/ref|> <|box|> (26,44),(72,60) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (26,44),(71,60) <|/box|> <|ref|> Pleural thickening <|/ref|> <|box|> (28,53),(34,61) <|/box|> <|ref|> Pleural effusion <|/ref|> <|box|> (28,53),(34,61) <|/box|> <|ref|> Pleural thickening <|/ref|> <|box|> (28,53),(34,61) <|/box|> <|ref|> Pleural effusion <|/ref|> <|box|> (28,53),(34,61) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (28,53),(34,61) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40078.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40786.png\nGenerated Report:\n\n<|ref|> Lung Opacity <|/ref|> <|box|> (10,54),(38,63) <|/box|> <|ref|> Pleural Effusion <|/ref|> <|box|> (10,54),(38,63) <|/box|> <|ref|> Cardiomegaly <|/ref|> <|box|> (36,50),(80,64) <|/box|> <|ref|> Aortic enlargement <|/ref|> <|box|> (50,29),(62,40) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40786.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/39069.png\nGenerated Report:\n\n<|ref|> Infiltration <|/ref|> <|box|> (18,20),(43,44) <|/box|> <|ref|> Infiltration <|/ref|> <|box|> (58,16),(85,44) <|/box|> <|ref|> Lung Opacity <|/ref|> <|box|> (60,44),(82,56) <|/box|> <|ref|> Lung Opacity <|/ref|> <|box|> (71,47),(85,56) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/39069.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/39099.png\nGenerated Report:\n\n<|ref|> Airway: Clips <|/ref|> <|box|> (38,24),(46,31) <|/box|> <|ref|> Mediastinal: Clips <|/ref|> <|box|> (47,24),(56,31) <|/box|> <|ref|> Lungs: Lung volumes are low.**\nSaved boxed image to /kaggle/working/out_chestx-det10/39099.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40841.png\nGenerated Report:\n\n<|ref|> Cardiomegaly <|/ref|> <|box|> (38,50),(80,64) <|/box|> <|ref|> Aortic enlargement <|/ref|> <|box|> (50,27),(62,38) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (76,38),(92,64) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40841.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40873.png\nGenerated Report:\n\n<|ref|> Other lesion <|/ref|> <|box|> (4,34),(44,73) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (50,34),(91,73) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (51,34),(91,73) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40873.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41760.png\nGenerated Report:\n\n<|ref|> Cardiomegaly <|/ref|> <|box|> (38,44),(80,60) <|/box|> <|ref|> Pulmonary fibrosis <|/ref|> <|box|> (74,47),(83,50) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41760.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/39524.png\nGenerated Report:\n\n<|ref|> Airway and mediastinal contours are unremarkable.**\nSaved boxed image to /kaggle/working/out_chestx-det10/39524.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41783.png\nGenerated Report:\n\n<|ref|> Airway and mediastinal shift <|/ref|> <|box|> (47,20),(62,40) <|/box|> <|ref|> Lung Opacity <|/ref|> <|box|> (60,46),(85,64) <|/box|> <|ref|> Pleural Effusion <|/ref|> <|box|> (60,46),(85,64) <|/box|> <|ref|> Pneumothorax <|/ref|> <|box|> (66,50),(76,60) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41783.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/36365.png\nGenerated Report:\n\n<|ref|> Airway Opacity <|/ref|> <|box|> (60,37),(73,50) <|/box|> <|ref|> Lung Opacity <|/ref|> <|box|> (60,37),(73,50) <|/box|> <|ref|> Nodule/Mass <|/ref|> <|box|> (60,37),(73,50) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/36365.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/39996.png\nGenerated Report:\n\n<|ref|> Pneumothorax <|/ref|> <|box|> (21,5),(46,22) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (21,5),(46,22) <|/box|> <|ref|> Aortic enlargement <|/ref|> <|box|> (50,22),(62,34) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/39996.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41221.png\nGenerated Report:\n\n<|ref|> Cardiomegaly <|/ref|> <|box|> (30,41),(72,56) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (52,44),(86,60) <|/box|> <|ref|> Pleural effusion <|/ref|> <|box|> (54,52),(63,60) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (55,44),(86,60) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41221.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40038.png\nGenerated Report:\n\n<|ref|> Infiltration <|/ref|> <|box|> (20,28),(40,46) <|/box|> <|ref|> Cardiomegaly <|/ref|> <|box|> (40,36),(80,50) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (43,22),(67,67) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (44,22),(67,67) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (44,22),(67,67) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40038.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/39705.png\nGenerated Report:\n\nNo abnormalities detected.\nSaved boxed image to /kaggle/working/out_chestx-det10/39705.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41207.png\nGenerated Report:\n\n<|ref|> Cardiomegaly <|/ref|> <|box|> (34,41),(85,60) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (64,50),(73,56) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41207.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41084.png\nGenerated Report:\n\n<|ref|> Infiltration <|/ref|> <|box|> (20,28),(38,54) <|/box|> <|ref|> Infiltration <|/ref|> <|box|> (64,30),(82,56) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41084.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41850.png\nGenerated Report:\n\n<|ref|> Cardiomegaly <|/ref|> <|box|> (34,48),(80,68) <|/box|> <|ref|> Aortic enlargement <|/ref|> <|box|> (46,24),(59,36) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41850.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/39748.png\nGenerated Report:\n\n<|ref|> Airway and mediastinum <|/ref|> <|box|> (33,24),(46,46) <|/box|> <|ref|> Lung fields <|/ref|> <|box|> (33,24),(46,46) <|/box|> <|ref|> Cardiac silhouette <|/ref|> <|box|> (36,50),(73,64) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (36,50),(73,64) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/39748.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40027.png\nGenerated Report:\n\n<|ref|> Cardiomegaly <|/ref|> <|box|> (31,41),(84,68) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (40,25),(49,40) <|/box|> <|ref|> Aortic enlargement <|/ref|> <|box|> (48,30),(62,43) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40027.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40095.png\nGenerated Report:\n\n<|ref|> Infiltration <|/ref|> <|box|> (14,34),(30,52) <|/box|> <|ref|> Cardiomegaly <|/ref|> <|box|> (32,50),(80,68) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (33,28),(38,34) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (66,50),(74,60) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40095.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/39712.png\nGenerated Report:\n\n<|ref|> Infiltration <|/ref|> <|box|> (20,24),(38,44) <|/box|> <|ref|> Pulmonary fibrosis <|/ref|> <|box|> (20,24),(38,44) <|/box|> <|ref|> Pleural thickening <|/ref|> <|box|> (20,58),(26,63) <|/box|> <|ref|> Pleural effusion <|/ref|> <|box|> (20,58),(26,63) <|/box|> <|ref|> Pleural thickening <|/ref|> <|box|> (75,50),(85,60) <|/box|> <|ref|> Pleural effusion <|/ref|> <|box|> (75,50),(85,60) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/39712.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41771.png\nGenerated Report:\n\n<|ref|> Air-fluid level <|/ref|> <|box|> (59,50),(76,64) <|/box|> <|ref|> Pleural effusion <|/ref|> <|box|> (59,50),(76,64) <|/box|> <|ref|> Pneumothorax <|/ref|> <|box|> (59,50),(76,64) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41771.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40424.png\nGenerated Report:\n\n<|ref|> Infiltration <|/ref|> <|box|> (8,8),(45,77) <|/box|> <|ref|> Infiltration <|/ref|> <|box|> (54,8),(90,77) <|/box|> <|ref|> Mediastinal shift <|/ref|> <|box|> (54,8),(74,38) <|/box|> <|ref|> Mediastinal shift <|/ref|> <|box|> (55,20),(65,31) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (55,20),(65,31) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40424.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40774.png\nGenerated Report:\n\n<|ref|> Infiltration <|/ref|> <|box|> (14,46),(40,54) <|/box|> <|ref|> Infiltration <|/ref|> <|box|> (71,47),(90,54) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40774.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/39417.png\nGenerated Report:\n\n<|ref|> Cardiomegaly <|/ref|> <|box|> (38,50),(86,75) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/39417.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40749.png\nGenerated Report:\n\n<|ref|> Lung Opacity <|/ref|> <|box|> (60,24),(85,52) <|/box|> <|ref|> Pleural Effusion <|/ref|> <|box|> (82,60),(89,70) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40749.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41153.png\nGenerated Report:\n\n<|ref|> Airway and mediastinal shift <|/ref|> <|box|> (40,8),(54,46) <|/box|> <|ref|> Infiltration <|/ref|> <|box|> (60,12),(85,84) <|/box|> <|ref|> Lung Opacity <|/ref|> <|box|> (66,12),(85,84) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41153.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41165.png\nGenerated Report:\n\n<|ref|> Airway and mediastinum <|/ref|> <|box|> (40,28),(48,46) <|/box|> <|ref|> Lung Opacity <|/ref|> <|box|> (60,50),(73,64) <|/box|> <|ref|> Cardiomegaly <|/ref|> <|box|> (36,48),(80,64) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (60,50),(73,64) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41165.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41798.png\nGenerated Report:\n\n<|ref|> Lung Opacity <|/ref|> <|box|> (20,30),(38,56) <|/box|> <|ref|> Lung Opacity <|/ref|> <|box|> (60,30),(82,68) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41798.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40748.png\nGenerated Report:\n\n<|ref|> Infiltration <|/ref|> <|box|> (20,30),(40,64) <|/box|> <|ref|> Infiltration <|/ref|> <|box|> (60,30),(83,64) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40748.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40412.png\nGenerated Report:\n\n<|ref|> Airway Opacity <|/ref|> <|box|> (40,20),(48,40) <|/box|> <|ref|> Mediastinal Opacity <|/ref|> <|box|> (40,20),(48,40) <|/box|> <|ref|> Lung Opacity <|/ref|> <|box|> (60,50),(90,76) <|/box|> <|ref|> Cardiomegaly <|/ref|> <|box|> (64,50),(90,76) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40412.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40785.png\nGenerated Report:\n\n<|ref|> Other lesion <|/ref|> <|box|> (54,50),(64,60) <|/box|> <|ref|> Aortic enlargement <|/ref|> <|box|> (52,22),(62,32) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40785.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/36346.png\nGenerated Report:\n\n[Everything else: Tubes] The right PICC line has been repositioned and now ends in the mid SVC. [Breathing: Lungs] **There is a slight increase in pulmonary vascular congestion.** [Breathing: Lungs] **There is a slight increase in right basilar atelectasis.** [Breathing: Pleura] There is no pneumothorax. [Cardiac: Heart Size] **The cardiac silhouette is mildly enlarged.** [Everything else: Valves] The patient has undergone mitral and aortic valve replacements. [Everything else: Pacemakers] There are epicardial pacer leads.\nSaved boxed image to /kaggle/working/out_chestx-det10/36346.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/39790.png\nGenerated Report:\n\n<|ref|> Lung Opacity <|/ref|> <|box|> (14,50),(38,62) <|/box|> <|ref|> Pleural thickening <|/ref|> <|box|> (14,50),(38,62) <|/box|> <|ref|> Pleural effusion <|/ref|> <|box|> (14,50),(38,62) <|/box|> <|ref|> Pulmonary fibrosis <|/ref|> <|box|> (20,28),(34,38) <|/box|> <|ref|> Pulmonary fibrosis <|/ref|> <|box|> (66,20),(75,30) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/39790.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/39717.png\nGenerated Report:\n\n<|ref|> Infiltration <|/ref|> <|box|> (14,54),(30,70) <|/box|> <|ref|> Nodule/Mass <|/ref|> <|box|> (20,45),(27,50) <|/box|> <|ref|> Lung Opacity <|/ref|> <|box|> (20,45),(27,50) <|/box|> <|ref|> Aortic enlargement <|/ref|> <|box|> (54,28),(65,40) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/39717.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/36199.png\nGenerated Report:\n\n<|ref|> Airway and mediastinum <|/ref|> <|box|> (38,20),(50,36) <|/box|> <|ref|> Lung fields <|/ref|> <|box|> (64,48),(73,56) <|/box|> <|ref|> Cardiac silhouette <|/ref|> <|box|> (40,46),(73,60) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (64,52),(70,59) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/36199.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40887.png\nGenerated Report:\n\n<|ref|> Infiltration <|/ref|> <|box|> (20,14),(46,38) <|/box|> <|ref|> Pulmonary fibrosis <|/ref|> <|box|> (20,14),(46,38) <|/box|> <|ref|> Cardiomegaly <|/ref|> <|box|> (38,50),(76,64) <|/box|> <|ref|> Aortic enlargement <|/ref|> <|box|> (48,29),(60,40) <|/box|> <|ref|> Pleural thickening <|/ref|> <|box|> (55,10),(70,16) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40887.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/39473.png\nGenerated Report:\n\nNo abnormalities detected.\nSaved boxed image to /kaggle/working/out_chestx-det10/39473.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/39994.png\nGenerated Report:\n\n<|ref|> Airway and mediastinum <|/ref|> <|box|> (36,30),(48,46) <|/box|> <|ref|> Lung fields <|/ref|> <|box|> (36,30),(48,46) <|/box|> <|ref|> Cardiac silhouette <|/ref|> <|box|> (39,50),(76,68) <|/box|> <|ref|> Bones and soft tissues <|/ref|> <|box|> (39,50),(76,68) <|/box|> <|ref|> foreign object <|/ref|> <|box|> (64,4),(73,20) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/39994.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40394.png\nGenerated Report:\n\nNo significant findings.\nSaved boxed image to /kaggle/working/out_chestx-det10/40394.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41110.png\nGenerated Report:\n\n<|ref|> Cardiomegaly <|/ref|> <|box|> (39,55),(81,72) <|/box|> <|ref|> Aortic enlargement <|/ref|> <|box|> (50,29),(62,40) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41110.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/39697.png\nGenerated Report:\n\n<|ref|> Infiltration <|/ref|> <|box|> (14,41),(38,56) <|/box|> <|ref|> Cardiomegaly <|/ref|> <|box|> (34,44),(81,63) <|/box|> <|ref|> Aortic enlargement <|/ref|> <|box|> (48,28),(60,38) <|/box|> <|ref|> Pleural thickening <|/ref|> <|box|> (59,10),(69,13) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/39697.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40422.png\nGenerated Report:\n\n<|ref|> Lung Opacity <|/ref|> <|box|> (55,12),(75,30) <|/box|> <|ref|> Pulmonary fibrosis <|/ref|> <|box|> (55,12),(75,30) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (55,12),(75,30) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40422.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40864.png\nGenerated Report:\n\nNo abnormalities detected.\nSaved boxed image to /kaggle/working/out_chestx-det10/40864.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/39404.png\nGenerated Report:\n\n<|ref|> Lung Opacity <|/ref|> <|box|> (15,50),(20,54) <|/box|> <|ref|> Lung Opacity <|/ref|> <|box|> (80,50),(86,56) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/39404.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41492.png\nGenerated Report:\n\n<|ref|> Infiltration <|/ref|> <|box|> (20,30),(40,68) <|/box|> <|ref|> Infiltration <|/ref|> <|box|> (60,32),(82,68) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41492.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/39946.png\nGenerated Report:\n\n<|ref|> Cardiomegaly <|/ref|> <|box|> (34,47),(80,73) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (39,50),(64,76) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (40,50),(64,76) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (40,50),(64,76) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (40,50),(64,76) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (40,50),(64,76) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/39946.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/39734.png\nGenerated Report:\n\n<|ref|> Lung Opacity <|/ref|> <|box|> (76,64),(82,69) <|/box|> <|ref|> Nodule/Mass <|/ref|> <|box|> (77,64),(82,69) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/39734.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41503.png\nGenerated Report:\n\n<|ref|> Infiltration <|/ref|> <|box|> (8,14),(43,80) <|/box|> <|ref|> Infiltration <|/ref|> <|box|> (54,14),(90,80) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41503.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/39133.png\nGenerated Report:\n\n<|ref|> Infiltration <|/ref|> <|box|> (56,24),(86,63) <|/box|> <|ref|> Lung Opacity <|/ref|> <|box|> (56,24),(86,63) <|/box|> <|ref|> Pneumothorax <|/ref|> <|box|> (56,24),(82,34) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/39133.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41139.png\nGenerated Report:\n\n<|ref|> Airway and mediastinum <|/ref|> <|box|> (28,30),(48,68) <|/box|> <|ref|> Lungs <|/ref|> <|box|> (60,34),(83,64) <|/box|> <|ref|> Cardiac silhouette <|/ref|> <|box|> (32,56),(75,76) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (64,64),(96,86) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41139.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41102.png\nGenerated Report:\n\n<|ref|> Infiltration <|/ref|> <|box|> (14,14),(40,47) <|/box|> <|ref|> Pulmonary fibrosis <|/ref|> <|box|> (14,14),(40,47) <|/box|> <|ref|> Pulmonary fibrosis <|/ref|> <|box|> (60,14),(85,47) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (60,14),(85,47) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41102.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41072.png\nGenerated Report:\n\n<|ref|> Infiltration <|/ref|> <|box|> (20,44),(40,56) <|/box|> <|ref|> Lung Opacity <|/ref|> <|box|> (20,44),(40,56) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (20,44),(40,56) <|/box|> <|ref|> Cardiomegaly <|/ref|> <|box|> (38,48),(74,61) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (38,48),(74,61) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41072.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40392.png\nGenerated Report:\n\n<|ref|> Infiltration <|/ref|> <|box|> (20,30),(38,46) <|/box|> <|ref|> Lung Opacity <|/ref|> <|box|> (20,30),(38,46) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (20,30),(38,46) <|/box|> <|ref|> Cardiomegaly <|/ref|> <|box|> (40,44),(76,56) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40392.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/39414.png\nGenerated Report:\n\n<|ref|> Infiltration <|/ref|> <|box|> (60,30),(73,44) <|/box|> <|ref|> Nodule/Mass <|/ref|> <|box|> (60,30),(73,44) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/39414.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41834.png\nGenerated Report:\n\nNo abnormalities detected.\nSaved boxed image to /kaggle/working/out_chestx-det10/41834.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41200.png\nGenerated Report:\n\n<|ref|> Lung Opacity <|/ref|> <|box|> (14,50),(30,64) <|/box|> <|ref|> Cardiomegaly <|/ref|> <|box|> (36,54),(80,72) <|/box|> <|ref|> Aortic enlargement <|/ref|> <|box|> (50,27),(60,36) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (74,64),(80,68) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41200.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40388.png\nGenerated Report:\n\nNo abnormalities detected.\nSaved boxed image to /kaggle/working/out_chestx-det10/40388.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40775.png\nGenerated Report:\n\n<|ref|> Lung Opacity <|/ref|> <|box|> (10,48),(40,56) <|/box|> <|ref|> Pulmonary fibrosis <|/ref|> <|box|> (20,48),(34,52) <|/box|> <|ref|> Cardiomegaly <|/ref|> <|box|> (36,44),(80,58) <|/box|> <|ref|> Aortic enlargement <|/ref|> <|box|> (50,27),(62,38) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40775.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40111.png\nGenerated Report:\n\n<|ref|> Airway and mediastinum <|/ref|> <|box|> (40,28),(60,45) <|/box|> <|ref|> Lung fields <|/ref|> <|box|> (67,30),(76,41) <|/box|> <|ref|> Cardiac silhouette <|/ref|> <|box|> (36,54),(78,72) <|/box|> <|ref|> Bones and soft tissues <|/ref|> <|box|> (36,54),(78,72) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40111.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41481.png\nGenerated Report:\n\nNo abnormalities detected.\nSaved boxed image to /kaggle/working/out_chestx-det10/41481.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/39430.png\nGenerated Report:\n\n<|ref|> Infiltration <|/ref|> <|box|> (14,20),(45,62) <|/box|> <|ref|> Consolidation <|/ref|> <|box|> (14,20),(45,62) <|/box|> <|ref|> Lung Opacity <|/ref|> <|box|> (14,20),(45,62) <|/box|> <|ref|> Consolidation <|/ref|> <|box|> (14,20),(45,62) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/39430.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40028.png\nGenerated Report:\n\n<|ref|> Cardiomegaly <|/ref|> <|box|> (31,41),(84,68) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40028.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41456.png\nGenerated Report:\n\n<|ref|> Lung Opacity <|/ref|> <|box|> (14,64),(16,66) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (14,64),(16,66) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (14,64),(16,66) <|/box|> <|ref|> Cardiomegaly <|/ref|> <|box|> (38,54),(76,66) <|/box|> <|ref|> Aortic enlargement <|/ref|> <|box|> (50,24),(60,34) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41456.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40802.png\nGenerated Report:\n\n<|ref|> Airway Opacity <|/ref|> <|box|> (31,40),(40,52) <|/box|> <|ref|> Mediastinal Contour <|/ref|> <|box|> (50,28),(62,40) <|/box|> <|ref|> Lung Opacity <|/ref|> <|box|> (60,50),(85,66) <|/box|> <|ref|> Pleural Effusion <|/ref|> <|box|> (60,50),(85,66) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40802.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/39066.png\nGenerated Report:\n\n<|ref|> Airway: Hilar Structures <|/ref|> <|box|> (35,38),(40,41) <|/box|> <|ref|> Cardiac: Heart Size and Borders <|/ref|> <|box|> (38,54),(78,72) <|/box|> <|ref|> Aortic enlargement <|/ref|> <|box|> (48,26),(63,41) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/39066.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41807.png\nGenerated Report:\n\nNo abnormalities detected.\nSaved boxed image to /kaggle/working/out_chestx-det10/41807.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40839.png\nGenerated Report:\n\n<|ref|> Lung Opacity <|/ref|> <|box|> (20,48),(38,56) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (20,48),(38,56) <|/box|> <|ref|> Pleural thickening <|/ref|> <|box|> (20,48),(38,56) <|/box|> <|ref|> Pleural effusion <|/ref|> <|box|> (20,48),(38,56) <|/box|> <|ref|> Pleural thickening <|/ref|> <|box|> (20,48),(38,56) <|/box|> <|ref|> Pleural effusion <|/ref|> <|box|> (20,48),(38,56) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40839.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/39408.png\nGenerated Report:\n\n<|ref|> Lung Opacity <|/ref|> <|box|> (20,50),(40,63) <|/box|> <|ref|> Cardiomegaly <|/ref|> <|box|> (36,54),(82,72) <|/box|> <|ref|> Aortic enlargement <|/ref|> <|box|> (50,29),(66,44) <|/box|> <|ref|> Lung Opacity <|/ref|> <|box|> (66,50),(86,66) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/39408.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41460.png\nGenerated Report:\n\n<|ref|> Airway and mediastinum <|/ref|> <|box|> (44,14),(56,31) <|/box|> <|ref|> Lung fields <|/ref|> <|box|> (64,28),(73,38) <|/box|> <|ref|> Cardiac silhouette <|/ref|> <|box|> (44,50),(81,72) <|/box|> <|ref|> Bones and soft tissues <|/ref|> <|box|> (44,50),(81,72) <|/box|> <|ref|> foreign object <|/ref|> <|box|> (50,6),(72,23) <|/box|> <|ref|> foreign object <|/ref|> <|box|> (76,50),(92,72) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41460.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/39140.png\nGenerated Report:\n\n<|ref|> Lung Opacity <|/ref|> <|box|> (20,12),(44,30) <|/box|> <|ref|> Nodule/Mass <|/ref|> <|box|> (20,12),(44,30) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (20,12),(44,30) <|/box|> <|ref|> Aortic enlargement <|/ref|> <|box|> (48,29),(60,41) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/39140.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40797.png\nGenerated Report:\n\n<|ref|> Aortic enlargement <|/ref|> <|box|> (50,30),(63,45) <|/box|> <|ref|> Pleural effusion <|/ref|> <|box|> (75,60),(96,81) <|/box|> <|ref|> Pleural thickening <|/ref|> <|box|> (80,68),(87,78) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40797.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40083.png\nGenerated Report:\n\n<|ref|> Cardiomegaly <|/ref|> <|box|> (38,47),(85,65) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (40,28),(58,84) <|/box|> <|ref|> Pneumothorax <|/ref|> <|box|> (26,17),(44,24) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (66,50),(73,56) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40083.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41101.png\nGenerated Report:\n\n<|ref|> Airway and mediastinum <|/ref|> <|box|> (34,24),(46,36) <|/box|> <|ref|> Lung fields <|/ref|> <|box|> (66,28),(74,36) <|/box|> <|ref|> Cardiac silhouette <|/ref|> <|box|> (36,40),(73,54) <|/box|> <|ref|> Bones and soft tissues <|/ref|> <|box|> (36,20),(48,28) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (36,20),(48,28) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41101.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40473.png\nGenerated Report:\n\n<|ref|> Infiltration <|/ref|> <|box|> (20,26),(40,54) <|/box|> <|ref|> Infiltration <|/ref|> <|box|> (60,30),(80,54) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40473.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40762.png\nGenerated Report:\n\n<|ref|> Airway: Endotracheal tube is not visualized. <|/ref|> <|box|> (34,44),(45,60) <|/box|> <|ref|> Lungs: **There is a persistent left retrocardiac opacity.**\nSaved boxed image to /kaggle/working/out_chestx-det10/40762.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/39468.png\nGenerated Report:\n\n<|ref|> Lung Opacity <|/ref|> <|box|> (28,14),(34,18) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (28,14),(34,18) <|/box|> <|ref|> Pleural thickening <|/ref|> <|box|> (59,6),(69,10) <|/box|> <|ref|> Pleural thickening <|/ref|> <|box|> (89,80),(94,87) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/39468.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/36266.png\nGenerated Report:\n\n<|ref|> Lung Opacity <|/ref|> <|box|> (64,34),(82,48) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/36266.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/39511.png\nGenerated Report:\n\n<|ref|> Lung Opacity <|/ref|> <|box|> (14,50),(38,68) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (50,54),(85,72) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (75,50),(85,64) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (76,44),(85,50) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/39511.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40360.png\nGenerated Report:\n\n<|ref|> Infiltration <|/ref|> <|box|> (20,8),(46,54) <|/box|> <|ref|> Infiltration <|/ref|> <|box|> (58,8),(85,54) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (58,8),(85,54) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (59,26),(68,38) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40360.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41128.png\nGenerated Report:\n\n<|ref|> Airway and mediastinum <|/ref|> <|box|> (33,30),(46,46) <|/box|> <|ref|> Lung fields <|/ref|> <|box|> (34,30),(44,44) <|/box|> <|ref|> Cardiac silhouette <|/ref|> <|box|> (36,44),(76,61) <|/box|> <|ref|> Bones and soft tissues <|/ref|> <|box|> (36,44),(76,61) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (36,44),(76,61) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41128.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41069.png\nGenerated Report:\n\n<|ref|> Airway and mediastinum <|/ref|> <|box|> (40,24),(54,40) <|/box|> <|ref|> Lungs <|/ref|> <|box|> (40,24),(54,40) <|/box|> <|ref|> Cardiac silhouette <|/ref|> <|box|> (44,44),(81,56) <|/box|> <|ref|> Lungs <|/ref|> <|box|> (44,44),(81,56) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41069.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/36331.png\nGenerated Report:\n\n<|ref|> Lung Opacity <|/ref|> <|box|> (64,50),(73,60) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (64,50),(73,60) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/36331.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41518.png\nGenerated Report:\n\n<|ref|> Airway and mediastinal abnormalities <|/ref|> <|box|> (22,14),(44,45) <|/box|> <|ref|> Lung Opacity <|/ref|> <|box|> (22,14),(44,45) <|/box|> <|ref|> Pulmonary fibrosis <|/ref|> <|box|> (22,14),(44,45) <|/box|> <|ref|> Pulmonary fibrosis <|/ref|> <|box|> (58,16),(85,47) <|/box|> <|ref|> Calcification <|/ref|> <|box|> (59,16),(85,47) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41518.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40001.png\nGenerated Report:\n\n<|ref|> Lung Opacity <|/ref|> <|box|> (20,54),(40,68) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (20,54),(40,68) <|/box|> <|ref|> Cardiomegaly <|/ref|> <|box|> (28,54),(78,72) <|/box|> <|ref|> Aortic enlargement <|/ref|> <|box|> (47,29),(61,44) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40001.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/39968.png\nGenerated Report:\n\n<|ref|> Airway Opacity <|/ref|> <|box|> (34,8),(46,33) <|/box|> <|ref|> Mediastinal Opacity <|/ref|> <|box|> (52,8),(66,33) <|/box|> <|ref|> Lung Opacity <|/ref|> <|box|> (55,8),(73,33) <|/box|> <|ref|> Pleural Effusion <|/ref|> <|box|> (77,87),(90,99) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/39968.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40439.png\nGenerated Report:\n\n<|ref|> Airway and mediastinum <|/ref|> <|box|> (40,22),(52,38) <|/box|> <|ref|> Lung Opacity <|/ref|> <|box|> (60,14),(85,38) <|/box|> <|ref|> Pleural Effusion <|/ref|> <|box|> (60,14),(85,38) <|/box|> <|ref|> Pleural thickening <|/ref|> <|box|> (66,14),(85,38) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40439.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/41197.png\nGenerated Report:\n\n<|ref|> Lung Opacity <|/ref|> <|box|> (14,46),(38,60) <|/box|> <|ref|> Cardiomegaly <|/ref|> <|box|> (36,48),(85,64) <|/box|> <|ref|> Aortic enlargement <|/ref|> <|box|> (52,24),(63,34) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/41197.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40108.png\nGenerated Report:\n\n<|ref|> foreign object <|/ref|> <|box|> (12,20),(26,33) <|/box|> <|ref|> foreign object <|/ref|> <|box|> (28,1),(38,12) <|/box|> <|ref|> foreign object <|/ref|> <|box|> (34,0),(40,10) <|/box|> <|ref|> foreign object <|/ref|> <|box|> (34,1),(40,10) <|/box|> <|ref|> foreign object <|/ref|> <|box|> (66,1),(74,12) <|/box|> <|ref|> foreign object <|/ref|> <|box|> (76,20),(88,33) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40108.png\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/chestx-det10/40787.png\nGenerated Report:\n\n<|ref|> Lung Opacity <|/ref|> <|box|> (14,50),(40,60) <|/box|> <|ref|> Pleural Effusion <|/ref|> <|box|> (14,50),(40,60) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (14,50),(40,60) <|/box|> <|ref|> Pulmonary fibrosis <|/ref|> <|box|> (74,51),(80,54) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40787.png\n\nImage: /kaggle/input/chestx-det10/40421.png\nGenerated Report:\n\n<|ref|> Airway and mediastinum <|/ref|> <|box|> (47,14),(60,31) <|/box|> <|ref|> Lung Opacity <|/ref|> <|box|> (56,42),(73,56) <|/box|> <|ref|> Other lesion <|/ref|> <|box|> (56,42),(73,56) <|/box|> <|ref|> Pulmonary fibrosis <|/ref|> <|box|> (74,42),(80,46) <|/box|>\nSaved boxed image to /kaggle/working/out_chestx-det10/40421.png\n","output_type":"stream"}],"execution_count":11}]}